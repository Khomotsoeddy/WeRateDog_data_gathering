{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35d01be6",
   "metadata": {},
   "source": [
    "## Reporting: wragle_report\n",
    "* Create a **300-600 word written report** called \"wrangle_report.pdf\" or \"wrangle_report.html\" that briefly describes your wrangling efforts. This is to be framed as an internal document."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb20c24",
   "metadata": {},
   "source": [
    "## Purpose of the project\n",
    "\n",
    "The purpose of the project is gather and wrangle data from twitter account called @WeRateDog\n",
    "\n",
    "### Project tasks\n",
    "\n",
    "<ul>\n",
    "    <li>Data Gathering</li>\n",
    "    <li>Assessing Data</li>\n",
    "    <li>Cleaning Data</li>\n",
    "    <li>Storing Data</li>\n",
    "    <li>Analyzing and Visualizing Data</li>\n",
    "</ul>\n",
    "\n",
    "### Data Gathering\n",
    "\n",
    "We got three datasets for this project\n",
    "\n",
    "1. image_predictions\n",
    "2. twitter_archive\n",
    "3. tweet_json\n",
    "\n",
    "##### image_predictions\n",
    "\n",
    "I imported the Python requests and os libraries and used the get() function of the requests library, I got the data through its url and saved it in a response variable. Using the Python with open function, I wrote the responseâ€™s content to a image_predictions.tsv file in the same working directory. I then read the downloaded image_predictions.tsv file into a dataframe named image_predictions.\n",
    "\n",
    "##### twitter-archive-enhanced\n",
    "\n",
    "This data was provided for this project. I then downloaded twitter-archive-enhanced.csv file then move it to the folders that I saved my work. I then imported the python pandas library as pd and used the pandas read_csv() function to read the twitter-archive-enhanced.csv file into a dataframe named twitter_archive.\n",
    "\n",
    "##### tweet_json\n",
    "\n",
    "The zipped file was available to be downloaded. Since my developer account was not approved, I decided to download tweet-json.zip, then unzip it and move tweet-json.txt to the folders that I saved my work. I then use open function and a for loop, to read the tweet-json.txt line by line and loaded each line as json file. I saved each id, retweet_count, favorite_count, followers_count and friends_count which I later converted to a dataframe named tweet_json.\n",
    "\n",
    "### Assessing Data\n",
    "\n",
    "\n",
    "**Visually:** I printed the three different dataframes individually in a jupiter notebook and scrolled through left and righ, up and down. Secondly, I visually assessed the csv files in Excel spreadsheet.\n",
    "\n",
    "**Programmatically:** I did various programmatic assessment with various python and pandas methods and functions such as .info(), .describe(), .isnull(), .head(), .tail(), .sample(), .duplicated(), .value_counts() and shape.\n",
    "\n",
    "### Cleaning Data\n",
    "\n",
    "\n",
    "**This part was divided into three parts: Define, Code and Test.**\n",
    "\n",
    "Before doing starting cleaning I made copy of each dataset\n",
    "\n",
    "I followed **Define, Code and Test** and made the following clean\n",
    "\n",
    "<ul>\n",
    "    <li>drop in_reply_to_status_id, in_reply_to_user_id, retweeted_status_id, retweeted_status_user_id, retweeted_status_timestamp, source, expanded_urls</li>\n",
    "    <li>convert \"doggo\", \"flooter\", \"pupper\" and \"puppo\" columns into one \"stage\" column, then drop the four columns.</li>\n",
    "    <li>Rename id column</li>\n",
    "    <li>find the list of rating_denominator that are not equal to 10, then drop those rows</li>\n",
    "    <li>transfer the data type</li>\n",
    "    <li>find rows that have three false and drop them.</li>\n",
    "    <li>Rename all values in the name column that are not real name.</li>\n",
    "    <li>Merge the three dataframes to become one dataframe and merge them on tweet_id column</li>\n",
    "</ul>\n",
    "\n",
    "### Storing Data\n",
    "\n",
    "After gathering, assessing and cleaning the data, I saved the merged data in a csv file named twitter_archive_master.csv.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
